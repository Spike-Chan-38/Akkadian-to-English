{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Akkadian to English Transformer (From Scratch)\\n\n",
        "Notebook version of `core_training.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7adf57",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "SOURCE_COLUMN_CANDIDATES = [\"sentence_transliteration\", \"source\", \"akkadian\", \"transliteration\", \"text\"]\n",
        "TARGET_COLUMN_CANDIDATES = [\"sentence_translation\", \"target\", \"english\", \"translation\", \"label\"]\n",
        "KEY_COLUMN_CANDIDATES = [\"oare_id\", \"id\", \"sentence_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb63809",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5585930d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    src_vocab_size: int = 12000\n",
        "    tgt_vocab_size: int = 12000\n",
        "    d_model: int = 256\n",
        "    n_heads: int = 4\n",
        "    n_encoder_layers: int = 4\n",
        "    n_decoder_layers: int = 4\n",
        "    d_ff: int = 1024\n",
        "    dropout: float = 0.1\n",
        "    max_len: int = 256\n",
        "\n",
        "    pad_id: int = 0\n",
        "    bos_id: int = 1\n",
        "    eos_id: int = 2\n",
        "    unk_id: int = 3\n",
        "\n",
        "    batch_size: int = 32\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_steps: int = 2000\n",
        "    epochs: int = 15\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    val_ratio: float = 0.1\n",
        "    seed: int = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aca7b98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d722459c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[`]\", \"'\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def _resolve_path(path_str: str) -> Path:\n",
        "    path = Path(path_str)\n",
        "    if path.exists():\n",
        "        return path\n",
        "    script_dir = Path(__file__).resolve().parent\n",
        "    candidate = script_dir / path_str\n",
        "    if candidate.exists():\n",
        "        return candidate\n",
        "    raise FileNotFoundError(f\"Could not find file: {path_str}. Tried: '{path}' and '{candidate}'.\")\n",
        "\n",
        "\n",
        "def _pick_first_existing(columns: List[str], candidates: List[str]) -> str:\n",
        "    for name in candidates:\n",
        "        if name in columns:\n",
        "            return name\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def load_parallel_data(train_csv: str, sentences_csv: str = \"\") -> pd.DataFrame:\n",
        "    train_path = _resolve_path(train_csv)\n",
        "    train_df = pd.read_csv(train_path)\n",
        "\n",
        "    # Preferred path: train.csv already contains source/target pairs.\n",
        "    src_col = _pick_first_existing(list(train_df.columns), SOURCE_COLUMN_CANDIDATES)\n",
        "    tgt_col = _pick_first_existing(list(train_df.columns), TARGET_COLUMN_CANDIDATES)\n",
        "\n",
        "    if src_col and tgt_col:\n",
        "        sentence_data = train_df[[src_col, tgt_col]].dropna()\n",
        "    else:\n",
        "        if not sentences_csv:\n",
        "            raise ValueError(\n",
        "                \"Could not find source/target columns in train CSV and no supplementary \"\n",
        "                \"--sentences_csv was provided.\\n\"\n",
        "                f\"train columns: {list(train_df.columns)}\"\n",
        "            )\n",
        "\n",
        "        sentences_path = _resolve_path(sentences_csv)\n",
        "        sentences_df = pd.read_csv(sentences_path)\n",
        "        sent_src_col = _pick_first_existing(list(sentences_df.columns), SOURCE_COLUMN_CANDIDATES)\n",
        "        sent_tgt_col = _pick_first_existing(list(sentences_df.columns), TARGET_COLUMN_CANDIDATES)\n",
        "        sent_key_col = _pick_first_existing(list(sentences_df.columns), KEY_COLUMN_CANDIDATES)\n",
        "        train_key_col = _pick_first_existing(list(train_df.columns), KEY_COLUMN_CANDIDATES)\n",
        "\n",
        "        if sent_src_col and sent_tgt_col:\n",
        "            sentence_data = sentences_df[[sent_src_col, sent_tgt_col]].dropna()\n",
        "            src_col, tgt_col = sent_src_col, sent_tgt_col\n",
        "        elif sent_key_col and train_key_col and sent_key_col == train_key_col:\n",
        "            data = pd.merge(sentences_df, train_df, on=sent_key_col, how=\"left\")\n",
        "            src_col = _pick_first_existing(list(data.columns), SOURCE_COLUMN_CANDIDATES)\n",
        "            tgt_col = _pick_first_existing(list(data.columns), TARGET_COLUMN_CANDIDATES)\n",
        "            if not src_col or not tgt_col:\n",
        "                raise ValueError(\n",
        "                    \"Could not find source/target columns after merge. \"\n",
        "                    f\"Available columns: {list(data.columns)}\"\n",
        "                )\n",
        "            sentence_data = data[[src_col, tgt_col]].dropna()\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Could not build parallel data. \"\n",
        "                \"Need either source/target columns directly in train/sentences CSV, \"\n",
        "                \"or a shared key (e.g., oare_id) in both CSVs.\\n\"\n",
        "                f\"train columns: {list(train_df.columns)}\\n\"\n",
        "                f\"sentences columns: {list(sentences_df.columns)}\"\n",
        "            )\n",
        "\n",
        "    sentence_data = sentence_data.rename(columns={src_col: \"source\", tgt_col: \"target\"})\n",
        "\n",
        "    sentence_data[\"source\"] = sentence_data[\"source\"].apply(normalize_text)\n",
        "    sentence_data[\"target\"] = sentence_data[\"target\"].apply(normalize_text)\n",
        "\n",
        "    sentence_data = sentence_data[(sentence_data[\"source\"].str.len() > 0) & (sentence_data[\"target\"].str.len() > 0)]\n",
        "    sentence_data = sentence_data.drop_duplicates().reset_index(drop=True)\n",
        "    return sentence_data\n",
        "\n",
        "\n",
        "def load_test_data(train_csv: str, test_csv: str = \"\") -> Tuple[pd.DataFrame, str]:\n",
        "    if test_csv:\n",
        "        test_path = _resolve_path(test_csv)\n",
        "    else:\n",
        "        train_path = _resolve_path(train_csv)\n",
        "        candidate = train_path.parent / \"test.csv\"\n",
        "        if not candidate.exists():\n",
        "            raise FileNotFoundError(\n",
        "                \"Could not infer test.csv location from train.csv directory. \"\n",
        "                \"Pass --test_csv explicitly.\"\n",
        "            )\n",
        "        test_path = candidate\n",
        "\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    src_col = _pick_first_existing(list(test_df.columns), SOURCE_COLUMN_CANDIDATES)\n",
        "    if not src_col:\n",
        "        raise ValueError(\n",
        "            \"Could not find source column in test CSV. \"\n",
        "            f\"Available columns: {list(test_df.columns)}\"\n",
        "        )\n",
        "\n",
        "    out_df = test_df.copy()\n",
        "    out_df[\"source\"] = out_df[src_col].astype(str).apply(normalize_text)\n",
        "    out_df = out_df[out_df[\"source\"].str.len() > 0].reset_index(drop=True)\n",
        "    return out_df, src_col\n",
        "\n",
        "\n",
        "def split_train_val(df: pd.DataFrame, val_ratio: float, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    idx = list(range(len(df)))\n",
        "    rng = random.Random(seed)\n",
        "    rng.shuffle(idx)\n",
        "    val_size = int(len(idx) * val_ratio)\n",
        "    val_idx = set(idx[:val_size])\n",
        "    train_rows, val_rows = [], []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        if i in val_idx:\n",
        "            val_rows.append(row)\n",
        "        else:\n",
        "            train_rows.append(row)\n",
        "\n",
        "    train_df = pd.DataFrame(train_rows).reset_index(drop=True)\n",
        "    val_df = pd.DataFrame(val_rows).reset_index(drop=True)\n",
        "    return train_df, val_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e88a46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Tokenizers (SentencePiece)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247e8e0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "def _write_text_file(path: Path, lines: List[str]) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for line in lines:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "\n",
        "def train_sentencepiece(texts: List[str], model_prefix: str, vocab_size: int, cfg: Config) -> str:\n",
        "    txt_path = Path(model_prefix + \".txt\")\n",
        "    _write_text_file(txt_path, texts)\n",
        "\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=str(txt_path),\n",
        "        model_prefix=model_prefix,\n",
        "        model_type=\"bpe\",\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1.0,\n",
        "        pad_id=cfg.pad_id,\n",
        "        bos_id=cfg.bos_id,\n",
        "        eos_id=cfg.eos_id,\n",
        "        unk_id=cfg.unk_id,\n",
        "        input_sentence_size=200000,\n",
        "        shuffle_input_sentence=True,\n",
        "        # Important for low-resource corpora: do not crash when requested vocab is too high.\n",
        "        hard_vocab_limit=False,\n",
        "    )\n",
        "    return model_prefix + \".model\"\n",
        "\n",
        "\n",
        "def encode_text(sp: spm.SentencePieceProcessor, text: str, max_len: int, bos_id: int, eos_id: int) -> List[int]:\n",
        "    ids = sp.encode(text, out_type=int)\n",
        "    ids = ids[: max_len - 2]\n",
        "    return [bos_id] + ids + [eos_id]\n",
        "\n",
        "\n",
        "class ParallelTextDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, src_sp: spm.SentencePieceProcessor, tgt_sp: spm.SentencePieceProcessor, cfg: Config):\n",
        "        self.src = df[\"source\"].tolist()\n",
        "        self.tgt = df[\"target\"].tolist()\n",
        "        self.src_sp = src_sp\n",
        "        self.tgt_sp = tgt_sp\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        src_ids = encode_text(self.src_sp, self.src[idx], self.cfg.max_len, self.cfg.bos_id, self.cfg.eos_id)\n",
        "        tgt_ids = encode_text(self.tgt_sp, self.tgt[idx], self.cfg.max_len, self.cfg.bos_id, self.cfg.eos_id)\n",
        "        return {\n",
        "            \"src_ids\": src_ids,\n",
        "            \"tgt_ids\": tgt_ids,\n",
        "        }\n",
        "\n",
        "\n",
        "class InferenceTextDataset(Dataset):\n",
        "    def __init__(self, sources: List[str], src_sp: spm.SentencePieceProcessor, cfg: Config):\n",
        "        self.sources = sources\n",
        "        self.src_sp = src_sp\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        src_ids = encode_text(self.src_sp, self.sources[idx], self.cfg.max_len, self.cfg.bos_id, self.cfg.eos_id)\n",
        "        return src_ids\n",
        "\n",
        "\n",
        "def collate_fn(batch, pad_id: int):\n",
        "    src_max = max(len(x[\"src_ids\"]) for x in batch)\n",
        "    tgt_max = max(len(x[\"tgt_ids\"]) for x in batch)\n",
        "\n",
        "    src_batch = []\n",
        "    tgt_batch = []\n",
        "    for item in batch:\n",
        "        src = item[\"src_ids\"] + [pad_id] * (src_max - len(item[\"src_ids\"]))\n",
        "        tgt = item[\"tgt_ids\"] + [pad_id] * (tgt_max - len(item[\"tgt_ids\"]))\n",
        "        src_batch.append(src)\n",
        "        tgt_batch.append(tgt)\n",
        "\n",
        "    src_tensor = torch.tensor(src_batch, dtype=torch.long)\n",
        "    tgt_tensor = torch.tensor(tgt_batch, dtype=torch.long)\n",
        "    return src_tensor, tgt_tensor\n",
        "\n",
        "\n",
        "def collate_src_fn(batch, pad_id: int):\n",
        "    src_max = max(len(x) for x in batch)\n",
        "    src_batch = [x + [pad_id] * (src_max - len(x)) for x in batch]\n",
        "    return torch.tensor(src_batch, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084e97c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8ac050",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.pe[:, : x.size(1)]\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.src_emb = nn.Embedding(cfg.src_vocab_size, cfg.d_model, padding_idx=cfg.pad_id)\n",
        "        self.tgt_emb = nn.Embedding(cfg.tgt_vocab_size, cfg.d_model, padding_idx=cfg.pad_id)\n",
        "        self.pos = PositionalEncoding(cfg.d_model, cfg.max_len)\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=cfg.d_model,\n",
        "            nhead=cfg.n_heads,\n",
        "            num_encoder_layers=cfg.n_encoder_layers,\n",
        "            num_decoder_layers=cfg.n_decoder_layers,\n",
        "            dim_feedforward=cfg.d_ff,\n",
        "            dropout=cfg.dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(cfg.d_model, cfg.tgt_vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src_ids: torch.Tensor,\n",
        "        tgt_in_ids: torch.Tensor,\n",
        "        src_key_padding_mask: torch.Tensor,\n",
        "        tgt_key_padding_mask: torch.Tensor,\n",
        "        tgt_mask: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        src = self.dropout(self.pos(self.src_emb(src_ids) * math.sqrt(self.cfg.d_model)))\n",
        "        tgt = self.dropout(self.pos(self.tgt_emb(tgt_in_ids) * math.sqrt(self.cfg.d_model)))\n",
        "\n",
        "        out = self.transformer(\n",
        "            src=src,\n",
        "            tgt=tgt,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask,\n",
        "        )\n",
        "        logits = self.lm_head(out)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7bd14ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Train / Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b054a06a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "def make_tgt_mask(tgt_len: int, device: torch.device) -> torch.Tensor:\n",
        "    mask = torch.triu(torch.ones(tgt_len, tgt_len, device=device), diagonal=1)\n",
        "    return mask.masked_fill(mask == 1, float(\"-inf\"))\n",
        "\n",
        "\n",
        "def make_scheduler(optimizer, warmup_steps: int):\n",
        "    def lr_lambda(step: int):\n",
        "        if step < warmup_steps:\n",
        "            return float(step + 1) / max(1, warmup_steps)\n",
        "        return (warmup_steps ** 0.5) / ((step + 1) ** 0.5)\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scheduler, criterion, device, cfg: Config):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for src_ids, tgt_ids in loader:\n",
        "        src_ids = src_ids.to(device)\n",
        "        tgt_ids = tgt_ids.to(device)\n",
        "\n",
        "        tgt_in = tgt_ids[:, :-1]\n",
        "        tgt_out = tgt_ids[:, 1:]\n",
        "\n",
        "        src_pad = src_ids.eq(cfg.pad_id)\n",
        "        tgt_in_pad = tgt_in.eq(cfg.pad_id)\n",
        "        tgt_mask = make_tgt_mask(tgt_in.size(1), device)\n",
        "\n",
        "        logits = model(src_ids, tgt_in, src_pad, tgt_in_pad, tgt_mask)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / max(1, len(loader))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device, cfg: Config):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for src_ids, tgt_ids in loader:\n",
        "        src_ids = src_ids.to(device)\n",
        "        tgt_ids = tgt_ids.to(device)\n",
        "\n",
        "        tgt_in = tgt_ids[:, :-1]\n",
        "        tgt_out = tgt_ids[:, 1:]\n",
        "\n",
        "        src_pad = src_ids.eq(cfg.pad_id)\n",
        "        tgt_in_pad = tgt_in.eq(cfg.pad_id)\n",
        "        tgt_mask = make_tgt_mask(tgt_in.size(1), device)\n",
        "\n",
        "        logits = model(src_ids, tgt_in, src_pad, tgt_in_pad, tgt_mask)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / max(1, len(loader))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(model, src_ids, cfg: Config, device: torch.device, max_new_tokens: int = 64):\n",
        "    model.eval()\n",
        "\n",
        "    src_ids = src_ids.to(device)\n",
        "    src_pad = src_ids.eq(cfg.pad_id)\n",
        "\n",
        "    ys = torch.full((src_ids.size(0), 1), cfg.bos_id, dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        tgt_pad = ys.eq(cfg.pad_id)\n",
        "        tgt_mask = make_tgt_mask(ys.size(1), device)\n",
        "        logits = model(src_ids, ys, src_pad, tgt_pad, tgt_mask)\n",
        "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "        ys = torch.cat([ys, next_token], dim=1)\n",
        "        if (next_token == cfg.eos_id).all():\n",
        "            break\n",
        "\n",
        "    return ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c024e89e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe5c213",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "class Args:\n",
        "    train_csv = \"/kaggle/input/deep-past-initiative-machine-translation/train.csv\"\n",
        "    test_csv = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
        "    output_dir = \"/kaggle/working\"\n",
        "    predictions_csv = \"submission.csv\"\n",
        "    epochs = 15\n",
        "    batch_size = 32\n",
        "    max_len = 256\n",
        "    src_vocab_size = 12000\n",
        "    tgt_vocab_size = 12000\n",
        "\n",
        "\n",
        "args = Args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    cfg = Config(\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batch_size,\n",
        "        max_len=args.max_len,\n",
        "        src_vocab_size=args.src_vocab_size,\n",
        "        tgt_vocab_size=args.tgt_vocab_size,\n",
        "    )\n",
        "\n",
        "    set_seed(cfg.seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    output_dir = Path(args.output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    full_df = load_parallel_data(args.train_csv)\n",
        "    train_df, val_df = split_train_val(full_df, cfg.val_ratio, cfg.seed)\n",
        "    print(f\"Train rows: {len(train_df)} | Val rows: {len(val_df)}\")\n",
        "\n",
        "    print(\"Training SentencePiece tokenizers...\")\n",
        "    src_prefix = str(output_dir / \"spm_src\")\n",
        "    tgt_prefix = str(output_dir / \"spm_tgt\")\n",
        "    src_model_path = train_sentencepiece(train_df[\"source\"].tolist(), src_prefix, cfg.src_vocab_size, cfg)\n",
        "    tgt_model_path = train_sentencepiece(train_df[\"target\"].tolist(), tgt_prefix, cfg.tgt_vocab_size, cfg)\n",
        "\n",
        "    src_sp = spm.SentencePieceProcessor(model_file=src_model_path)\n",
        "    tgt_sp = spm.SentencePieceProcessor(model_file=tgt_model_path)\n",
        "\n",
        "    train_ds = ParallelTextDataset(train_df, src_sp, tgt_sp, cfg)\n",
        "    val_ds = ParallelTextDataset(val_df, src_sp, tgt_sp, cfg)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda b: collate_fn(b, cfg.pad_id),\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda b: collate_fn(b, cfg.pad_id),\n",
        "    )\n",
        "\n",
        "    model = Seq2SeqTransformer(cfg).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scheduler = make_scheduler(optimizer, cfg.warmup_steps)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=cfg.pad_id)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device, cfg)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device, cfg)\n",
        "        print(f\"Epoch {epoch}/{cfg.epochs} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            ckpt_path = output_dir / \"best_model.pt\"\n",
        "            torch.save({\"model_state_dict\": model.state_dict(), \"config\": cfg.__dict__}, ckpt_path)\n",
        "            print(f\"Saved best model to {ckpt_path}\")\n",
        "\n",
        "    if len(val_df) > 0:\n",
        "        print(\"Running a quick inference example from validation set...\")\n",
        "        sample_src = val_df.iloc[0][\"source\"]\n",
        "        src_ids = encode_text(src_sp, sample_src, cfg.max_len, cfg.bos_id, cfg.eos_id)\n",
        "        src_tensor = torch.tensor([src_ids], dtype=torch.long)\n",
        "\n",
        "        pred_ids = greedy_decode(model, src_tensor, cfg, device, max_new_tokens=64)[0].tolist()\n",
        "        pred_ids = [tok for tok in pred_ids if tok not in {cfg.pad_id, cfg.bos_id, cfg.eos_id}]\n",
        "        pred_text = tgt_sp.decode(pred_ids)\n",
        "\n",
        "        print(\"Source:\", sample_src)\n",
        "        print(\"Prediction:\", pred_text)\n",
        "        print(\"Reference:\", val_df.iloc[0][\"target\"])\n",
        "\n",
        "    print(\"Loading test data and generating predictions...\")\n",
        "    test_df, _ = load_test_data(args.train_csv, args.test_csv)\n",
        "    test_ds = InferenceTextDataset(test_df[\"source\"].tolist(), src_sp, cfg)\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda b: collate_src_fn(b, cfg.pad_id),\n",
        "    )\n",
        "\n",
        "    predictions = []\n",
        "    for src_batch in test_loader:\n",
        "        decoded = greedy_decode(model, src_batch, cfg, device, max_new_tokens=64).tolist()\n",
        "        for pred_ids in decoded:\n",
        "            pred_ids = [tok for tok in pred_ids if tok not in {cfg.pad_id, cfg.bos_id, cfg.eos_id}]\n",
        "            predictions.append(tgt_sp.decode(pred_ids))\n",
        "\n",
        "    id_col = _pick_first_existing(list(test_df.columns), KEY_COLUMN_CANDIDATES)\n",
        "    pred_df = pd.DataFrame({\"prediction\": predictions})\n",
        "    if id_col:\n",
        "        pred_df.insert(0, id_col, test_df[id_col].values)\n",
        "\n",
        "    pred_path = Path(args.predictions_csv)\n",
        "    if not pred_path.is_absolute():\n",
        "        pred_path = output_dir / pred_path\n",
        "    pred_df.to_csv(pred_path, index=False)\n",
        "    print(f\"Saved test predictions to {pred_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
